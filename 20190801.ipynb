{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 Word Representaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 장점\n",
    "\n",
    "- 가장 쉬운 방법\n",
    "- 여러 단어에 대한 1차원 벡터를 만들고 단어에 해당되는 부분을 1로 한다. 나오지 않으면 0\n",
    "\n",
    "#### 단점\n",
    "\n",
    "- 유사도를 고려하지 못함\n",
    "- 단어가 증가하게 되면 메모리를 너무 많이 쓴다.\n",
    "- 단어가 무한하기 때문에 모두 표현할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding이란?\n",
    "\n",
    "- 한 단어를 벡터 형태로 표현\n",
    "- 벡터로 표현하면 비슷한 단어를 비슷한 표현으로 나타낼 수 있다.(가까운 거리로)\n",
    "- 하나의 dimension이 아니라 다차원에 흩뿌려져 있다.\n",
    "- 컴퓨터가 단어의 의미를 이해하기 쉬워짐\n",
    "- one-hot보다 메모리 사이즈를 줄일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 의미는 주변 문맥을 통해 정해진다. 주변 context를 주고 단어를 예측하게 한다. 이런 식으로 학습시킨다. 반대로 중심단어로부터 주변 단어를 예측하게 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- distributed vector representation 학습기법\n",
    "- 주변 단어로부터 중심 단어를 학습시킨다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec의 특성\n",
    "\n",
    "- 공간에서 벡터 포인트간에 관계는 해당 단어들 간의 관계를 나타냄\n",
    "- man-woman, boy-girl 간의 차이가 비슷하다.\n",
    "- 수많은 의미론적 관계를 나타낼 수 있다.\n",
    "- 단어의 벡터를 이용해서 단어끼리 연산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 알고리즘\n",
    "\n",
    "- input: 주변단어\n",
    "- output: 중심단어\n",
    "- 오토인코더 모델과 비슷하다.\n",
    "- 디멘션을 줄였다가 다시 늘린다.(자기 자신이 나오도록)\n",
    "- window 사이즈를 정해주고 window 사이즈 내의 단어만 이용해서 각각의 확률을 높여주는 방식으로 학습시킨다.\n",
    "- 시퀀스는 고려하지 않는다.(조건부 확률 O)\n",
    "- 벡터 2개인 이유? optimization 쉬움. center, context마다 최적화해야하는 방법이 달라진다. 그래서 둘로 쪼개준다.\n",
    "- 2개의 모델\n",
    "    - skip-gram\n",
    "    - CBOW: context word들도 각각의 center embedding이 있다. 이를 모두 sum해준다.\n",
    "- negative sampling: context word 유사도를 증가시키고 해당되지 않는 word는 유사도를 줄여주는 것\n",
    "\n",
    "#### objective function\n",
    "\n",
    "- liklihood를 log한 것\n",
    "- liklihood를 최적화하는 것이 목적이다.\n",
    "\n",
    "#### 데이터를 만드는 방법\n",
    "\n",
    "- 문장이 주어지면 center word를 움직이면서 단어간의 관계를 나타낸 벡터를 만들어준다.\n",
    "- P(o|c)를 계산할 때 분모 계산 cost가 너무 크다. \n",
    "- 따라서 초반에는 단어가 너무 많으면 사용할 수 없었다.\n",
    "\n",
    "#### skip-gram with negative sampling\n",
    "\n",
    "- u_0: context embedding\n",
    "- v_c: center word embedding\n",
    "- k: negative sample 개수\n",
    "- u_j^T: negative sample embedding vec: 1로 가까워지도록 \n",
    "\n",
    "#### Singular Value Decomposition\n",
    "\n",
    "- 학습할 필요없고 바로 적용하면 나옴\n",
    "- 정보를 손실하지 않고 embedding vector를 구하는 것\n",
    "- 단점: count가 높은 것에 의존성이 높다. 해결하기 위해서 max count를 제한하고 너무 적게 나오면 무시, 윈도우 사이즈도 정해줌\n",
    "- count based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고 사항\n",
    "\n",
    "#### 단어를 학습하는 방법\n",
    "\n",
    "1. 중심을 통해 주변\n",
    "    - 원핫 인코딩을 한다.\n",
    "    - 이 방법은 성능이 낮은 편\n",
    "2. 주변을 통해 중심\n",
    "\n",
    "#### 오토인코더\n",
    "\n",
    "- curse of dimension: 데이터 공간이 커지게 되면 듬성듬성 존재해서 어떤 패턴을 찾기 힘들다.\n",
    "- 차원을 줄이는 것이 차원의 저주 때문에 의미가 있다.\n",
    "- 줄인 정보에는 원래 정보를 손상시키지 않는다. 따라서 원상 복구가 가능하다.\n",
    "- e.g. 2000 dims - 200 dims - 2000 dims\n",
    "\n",
    "#### Likelihood란?\n",
    "\n",
    "- 머신러닝은 liklihood를 높이는 방식으로 학습\n",
    "- 주어진 트레이닝 데이터를 얼마나 잘 모방했는지 지표\n",
    "- 주어진 확률들의 곱으로 나타냄. 하지만 계산을 편하게 하기 위해서 log를 해준다. log를 취하면 log의 합으로 표현 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN의 형태\n",
    "\n",
    "- cnn과 다른 점은 반복하는 부분\n",
    "- one to one은 vanila neural network나 cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "- 이전 state와 현재 input을 고려해서 현재 state 계산\n",
    "- turing machine과 비슷\n",
    "- 매 step마다 동일한 계산\n",
    "- one-hot을 그대로 넣진 않고 embedding으로 변환해서 인풋으로 넣어줌\n",
    "- training과 test 때 넣어주는 input sequence가 달라진다.\n",
    "- test할 땐 나온것을 바로 다음 스텝에 넣어주면 된다.\n",
    "- training할 땐 나온 것이 무엇이든 다음 스텝에선 ground truth를 인풋으로 넣어준다.\n",
    "- 인풋은 언제나 좋은 것이라고 가정한다. 이 때 인풋으로 이상한 값이 들어가면 그 이후 스텝은 모두 이상해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "\n",
    "- RNN은 시퀀스가 무한정 길어질 수 있기 때문에 메모리 문제가 발생할 수 있다. 그래서 학습할 수 있는만큼 잘라서 학습시키고 나온 것을 이용해서 다음 시퀀스 덩어리에 넘겨준다. Divide & Conquer처럼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 tanh를 주로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient Problem\n",
    "\n",
    "- W가 거의 0에 가까워서 gradient를 계속 구하면서 곱해지므로 점점 작아진다. 그래서 시퀀스가 점점 길어질수록 그레디언트 값이 멀리까지 전파되지 않는다.\n",
    "- RNN은 여러 방향으로부터 그레디언트를 받아서 흐르기 때문에 이를 평균(합치면)하면 값이 계속 커질 수도 있다. 그래서 vanishing 뿐만 아니라 exploding도 발생한다.\n",
    "- exploding은 clipping을 통해 해결한다. 최대값을 제한해주는 것(scaling down).\n",
    "- 하나의 스테이트에 모든 것을 담으려고 하는 것이 문제, 해결 방법은 memory를 따로 떼어 두는 것, 이렇게 해서 나온 것이 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "- 3개의 gate와 하나의 tanh\n",
    "- 좀 더 복잡한 모델에서 잘 작동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "\n",
    "- 메모리 적게 사용, 빠름\n",
    "- 처음에 GRU 써보고 안 되면 LSTM 써보는거 추천\n",
    "- residual connection으로 깊은 layer도 학습이 잘 되게 할 수 있다.(ResNet처럼)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM과 GRU 모두 exploding 발생은 어쩔 수 없다.  \n",
    "learning rate를 잘 조정해야한다.  \n",
    "RNN은 한 방향만 볼 수 있다는 단점이 있다. 해결책은 양방향. = Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN\n",
    "\n",
    "- concat해서 사용\n",
    "- 생성 task가 아니라면 bidirectional이 성능이 더 좋다.\n",
    "- forward와 backward의 파라미터는 각각 다르게 설정해야한다.\n",
    "- 전체 시퀀스를 접근할 수 있을 때 사용\n",
    "- 풍부한 feature을 만들어내기 위해서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer RNN\n",
    "\n",
    "- bidirectional은 끝까지 계산한 다음에 concat\n",
    "- 2개에서 4개 정도 쌓는다\n",
    "- skip connection을 쌓는다면 더 깊게도 가능\n",
    "- clipping은 필수\n",
    "- bidirectional을 쓸 수 있으면 반드시 쓰기\n",
    "- single보다 높은 성능 기대"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation\n",
    "\n",
    "- encoder: 주어진 문장의 정보를 의미를 매핑해주는 것(벡터로)\n",
    "- decoder: 매핑된 벡터를 바탕으로 생성(번역)해주는 것\n",
    "- many to many 기반\n",
    "- seq to seq model: 인코더와 디코더로 구성, 각 파라미터는 공유되지 않음\n",
    "- hidden state를 decoder의 초기로 initialize\n",
    "- nlp's special token\n",
    "    - star, sost: 문장의 시작\n",
    "    - end, eos: 문장의 끝\n",
    "    - pad: 아무 것도 없는 정보\n",
    "- 디코더에서 special token 중요, 문장의 처음과 끝을 start와 end token으로 알림\n",
    "- conditional language model: 상태가 주어지면 적절한 language 생성\n",
    "- 딥러닝 모델이 언제나 좋은 건 아니다. 적은 데이터의 경우 딥러닝이 나쁠 수 있다.\n",
    "- 인코더에서 모든 정보를 하나의 임베딩에 담는다면 디코더에서 생성할 때 정보를 접근하기 쉽지 않다. 그래서 attention을 통해 원하는 정보에 바로바로 접근할 수 있도록 제안됐다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "- 디코딩할 때 인코더에서 필요한 부분만 집중해서 정보를 뽑아오겠다.\n",
    "- 디코딩할 때 참조할 인코딩 정보를 softmax를 통해 계산해서 뽑아온다.\n",
    "\n",
    "### Mechanism\n",
    "\n",
    "- 인코딩 벡터를 만들어줌\n",
    "- 순서\n",
    "    - key와 query를 어떻게 다뤄줄지\n",
    "    - 유사도\n",
    "    - logit\n",
    "    - softmax\n",
    "    - context vector\n",
    "    - concat\n",
    "    - linear\n",
    "- rnn으로 문장을 hidden state로\n",
    "- hidden state를 decoder로 넘겨줌\n",
    "- decoder의 hidden state가 나옴, 이는 우리의 쿼리로 사용됨\n",
    "- 유사도 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention variants\n",
    "\n",
    "- basic dot product\n",
    "- multiplicative attention: bilinear attention,\n",
    "- additive attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concat = 정보를 덧셈해주는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention은 query를 기반으로 특정 value를 가져 오는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
