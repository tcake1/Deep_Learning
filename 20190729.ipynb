{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Perceptron\n",
    "\n",
    "인간의 뉴런을 모방했다. 그 가장 기본 단위를 perceptron이라고 한다.<br>\n",
    "\n",
    "## Single Layer Perceptron\n",
    "\n",
    "- AND Gate\n",
    "- OR Gate\n",
    "- XOR Gate: linearly separable problem\n",
    "\n",
    "## Multi Layer Perceptron\n",
    "\n",
    "- XOR Gate 문제 해결 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "- input layer\n",
    "- hidden layer\n",
    "- output layer\n",
    "\n",
    "Neural Network Playground: 뉴럴넷 쉽게 보여주는 사이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "각 연결의 가중치는 우리가 정해주는 것이 아니고 학습을 시켜서 최적의 값을 찾는 것이다.\n",
    "\n",
    "- $x_0$는 각 뉴런의 bias로 언제나 1로 해준다.\n",
    "- tanh는 0 부근에서 미분 불가능하다.\n",
    "- sigmoid는 미분 가능하고 0에서 1사이의 값을 갖는다. 확률과 비슷하다.\n",
    "- 내적\n",
    "- activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Example\n",
    "\n",
    "- handwritten digits for 0 to 9\n",
    "- train: 55,000, test: 10,000\n",
    "- 28 X 28의 행렬로 표현\n",
    "\n",
    "\n",
    "1. 행렬을 1차원 벡터로 flatten해서 인풋에 넣어준다.(764)\n",
    "2. hidden layer는 사용자가 알아서 설정한다.\n",
    "3. 가중치와 인풋을 내적해서 나온 값을 activation function을 통해 변환시킨다.<br>\n",
    "    중간 layer는 sigmoid는 거의 사용하지 않는다.\n",
    "3. output은 0부터 9가 될 확률을 반환한다.\n",
    "4. 평가 척도(loss function)를 만든다. <br>\n",
    "    $mse = (prediction - target)^2$: regression에서 주로 사용<br>\n",
    "    cross entropy loss, softmax cross entropy loss: classification에서 주로 사용\n",
    "6. loss를 떨어뜨리는 방법으로 학습시키는 것이 목표."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미분방정식을 풀기 어려운 경우, 그때 그때 가장 낮은 곳을 찾아서 내려가도록 하는 것이다.\n",
    "\n",
    "1. 아무대나 한곳에 점을 찍는다.\n",
    "2. 그 점에서 경사를 구한다.\n",
    "3. 그 경사값만큼 내려간다.\n",
    "4. 반복하다가 다시 올라가게 되면 그 점이 최솟값이 된다.\n",
    "\n",
    "딥러닝은 gradient descent로 loss function을 낮추는 식으로 가중치를 업데이트하는 것이 목적이다.<br>\n",
    "$\\theta_0 = \\theta_0 - \\alpha \\cdot \\frac {\\partial \\theta_0} {\\partial J}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropation using Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x, y, z) = (x + y)z$$\n",
    "$$q = x + y, \\frac {\\partial q} {\\partial x} = 1, \\frac {\\partial q} {\\partial y} = 1$$\n",
    "\n",
    "forward로 loss를 구하고 loss의 gradient를 구한 후 backprop로 가중치의 미분값까지 구하는게 한 과정이다.  \n",
    "learning rate를 적당히 잡아주는 것이 중요하다. 너무 작으면 학습이 너무 느리고 너무 커버리면 수렴하지 못하거나, local minima에 빠질 가능성이 있다.  \n",
    "backprop는 프레임워크에서 해주니까 몰라도 상관은 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU\n",
    "- 파이썬스러운 코딩이 쉬움\n",
    "- 가독성, 디버깅\n",
    "- Dynamic Network: 컴파일 안 하고도 결과값을 볼 수 있다. 모델 변경이 자유롭다. (cf. tensorflow는 static이라서 컴파일을 해야만 결과값을 알 수 있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본적으로 numpy array와 비슷\n",
    "- Gradient 정보값을 저장해 둠 (requires_grad = True인 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.FloatTensor 중간에 .cuda를 해야 GPU의 메모리에 올려줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "갖고 있는 데이터셋을 다룰 수 있게 해주는 인터페이스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural Network 관련된 클래스 제공\n",
    "- CNN\n",
    "- RNN\n",
    "- Regularization, Normalization, Dropout, BatchNorm2d 등도 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 관련된 기능 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클래스 형태로 사용한다. nn.Module이라는 최상위 부모 클래스를 상속 받아서 사용한다.\n",
    "- \\_\\_init\\_\\_(), forward()는 반드시 구현되어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 모델을 메모리나 GPU로 복사\n",
    "2. Optimization, Loss\n",
    "3. 학습 (순서를 동일하게 학습시키면 순서를 외울 수도 있으니까 순서는 랜덤하게 해준다.)\n",
    "4. 이전에 계산된 gradient는 계속 누적되기 때문에 이전 gradient 값을 제거해 주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test할 땐 gradient 계산할 필요 없다.\n",
    "- model.eval(): randomize한 것들을 고정시켜 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 자체를 저장하는 것이 아니고 모델의 상태를 저장, 변형에 유용\n",
    "- 불러올 땐 미리 모델 객체를 만들어주고, 상태를 불러와야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise: California housing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
